{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21af49",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '\"c:/Users/Arthur Mota/AppData/Local/Microsoft/WindowsApps/python3.11.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42840fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "TOLERANCIA_SINCRONIZACAO_S = 0.5     # Tolerância máxima (em seg) para juntar sensores\n",
    "FREQ_DOWNSAMPLE = '1s'          # '1min' = 1 ponto por minuto. Tente '10s' para 10 segundos.\n",
    "N_CLUSTERS = 3          # Número de clusters desejados\n",
    "PESSOA_INICIAL = 38      # Escolher a pessoa (exemplo)\n",
    "WINDOW_SIZE = 7           # Número de pontos por janela (tamanho da janela). Usar este valor para window_size em funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72970ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_dados_pessoa(pessoa_id):\n",
    "    # Caminhos dos arquivos\n",
    "    accel_file = f'acelerometro/acelerometro_{pessoa_id}.csv'\n",
    "    \n",
    "    gyro_file = f'giroscopio/giroscopio_{pessoa_id}.csv'\n",
    "    \n",
    "    # Carregar acelerômetro (CSV com cabeçalho)\n",
    "    df_accel = pd.read_csv(accel_file)\n",
    "    df_accel['timestamp'] = pd.to_datetime(df_accel['timestamp'])\n",
    "    \n",
    "    # Carregar giroscópio (TXT sem cabeçalho)\n",
    "    df_gyro = pd.read_csv(gyro_file)\n",
    "    df_gyro['timestamp'] = pd.to_datetime(df_gyro['timestamp'])\n",
    "    \n",
    "    return df_accel, df_gyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f2778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accel, df_gyro = carregar_dados_pessoa(38)\n",
    "print(df_gyro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sincronizar_dados(df_accel, df_gyro, TOLERANCIA_SINCRONIZACAO_S=None):\n",
    "    \"\"\"\n",
    "    Sincroniza dados de acelerômetro e giroscópio baseado em timestamps\n",
    "    e cria janelas de tempo com múltiplos pontos\n",
    "\n",
    "    Args:\n",
    "        TOLERANCIA_SINCRONIZACAO_S: (opcional) taxa de amostragem em segundos. Usada para ajustar a tolerância do merge_asof.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Dados sincronizados e agrupados\n",
    "    \"\"\"\n",
    "    # Merge dos dados usando o timestamp mais próximo\n",
    "    df_accel = df_accel.sort_values('timestamp').reset_index(drop=True)\n",
    "    df_gyro = df_gyro.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Determinar tolerância baseada na taxa de amostragem (se fornecida) ou usar 0.1s por padrão\n",
    "    if TOLERANCIA_SINCRONIZACAO_S is not None:\n",
    "        tol = pd.Timedelta(seconds=TOLERANCIA_SINCRONIZACAO_S)\n",
    "    else:\n",
    "        tol = pd.Timedelta(seconds=0.1)\n",
    "\n",
    "    # Combinar timestamps próximos\n",
    "    df_combined = pd.merge_asof(\n",
    "        df_accel, \n",
    "        df_gyro, \n",
    "        on='timestamp', \n",
    "        direction='nearest',\n",
    "        tolerance=tol,\n",
    "        suffixes=('', '_gyro')\n",
    "    )\n",
    "    \n",
    "    # Renomear colunas do giroscópio para gx, gy, gz\n",
    "    df_combined = df_combined.rename(columns={\n",
    "        'x_gyro': 'gx',\n",
    "        'y_gyro': 'gy',\n",
    "        'z_gyro': 'gz'\n",
    "    })\n",
    "    \n",
    "    df_combined = df_combined.dropna()\n",
    "    \n",
    "    return df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dados_sincronizados := sincronizar_dados(df_accel, df_gyro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69670b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 'dados_sincronizados' é o seu DataFrame da célula 6\n",
    "# print(f\"Pontos originais: {len(dados_sincronizados)}\")\n",
    "\n",
    "# # 1. Definir o timestamp como o índice\n",
    "# df_indexado = dados_sincronizados.set_index('timestamp')\n",
    "\n",
    "# # 2. Fazer o Downsample (Reamostragem)\n",
    "# # '1S' = agrupar dados em janelas de 1 segundo\n",
    "# # .mean() = calcular a média de todas as 7 leituras dentro daquele segundo\n",
    "# df_downsampled = df_indexado.resample('1T').mean()\n",
    "\n",
    "# # 3. Limpar linhas que possam ter ficado vazias (se houver buracos nos dados)\n",
    "# df_downsampled = df_downsampled.dropna()\n",
    "\n",
    "# # 4. (Opcional) Voltar o timestamp para uma coluna, se sua função 'criar_janelas'\n",
    "# #    não esperar que ele seja o índice.\n",
    "# df_downsampled = df_downsampled.reset_index()\n",
    "\n",
    "# print(f\"Pontos após downsample para 1Hz: {len(df_downsampled)}\")\n",
    "\n",
    "# # AGORA, você usaria 'df_downsampled' na sua função 'criar_features_janela'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_features_janela(df_downsampled, window_size):\n",
    "    \"\"\"\n",
    "    Cria features agrupando múltiplos pontos consecutivos em uma janela\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame com dados sincronizados\n",
    "        window_size: Número de pontos por janela (cluster point)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Features para clustering (cada linha = 1 janela com múltiplos pontos)\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    \n",
    "    # Percorrer os dados em janelas\n",
    "    for i in range(0, len(df_downsampled) - window_size + 1, window_size):\n",
    "        window = df_downsampled.iloc[i:i+window_size]\n",
    "        \n",
    "        # Concatenar todos os valores da janela em um único vetor de features\n",
    "        feature_vector = []\n",
    "        for _, row in window.iterrows():\n",
    "            # Adicionar acelerômetro (x, y, z)\n",
    "            feature_vector.extend([row['x'], row['y'], row['z']])\n",
    "            # Adicionar giroscópio (gx, gy, gz)\n",
    "            feature_vector.extend([row['gx'], row['gy'], row['gz']])\n",
    "        \n",
    "        features_list.append(feature_vector)\n",
    "    \n",
    "    return np.array(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lista = criar_features_janela(dados_sincronizados, WINDOW_SIZE)\n",
    "tam = len(features_lista[0])\n",
    "print(tam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_kmeans(features, n_clusters):\n",
    "    \"\"\"\n",
    "    Aplica K-means clustering nas features\n",
    "    \n",
    "    Args:\n",
    "        features: Array de features\n",
    "        n_clusters: Número de clusters\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (modelo KMeans treinado, labels dos clusters, features normalizadas)\n",
    "    \"\"\"\n",
    "    # Normalizar features\n",
    "    scaler = StandardScaler()\n",
    "    features_normalized = scaler.fit_transform(features)\n",
    "    \n",
    "    # Aplicar K-means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(features_normalized)\n",
    "    \n",
    "    return kmeans, labels, features_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e7e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_resultados(labels, pessoa_id, window_size):\n",
    "    \"\"\"\n",
    "    Plota os resultados do clustering\n",
    "    \n",
    "    Args:\n",
    "        labels: Labels preditos pelo K-means\n",
    "        pessoa_id: ID da pessoa\n",
    "        window_size: Tamanho da janela usada\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Criar eixo X (índice do conjunto de pontos)\n",
    "    x = np.arange(len(labels))\n",
    "    \n",
    "    # Identificar clusters únicos\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    n_clusters = len(unique)\n",
    "    \n",
    "    # Paleta de cores dinâmica\n",
    "    color_palette = plt.cm.tab10(np.linspace(0, 1, n_clusters))\n",
    "    color_map = {cluster_id: color_palette[i] for i, cluster_id in enumerate(unique)}\n",
    "    \n",
    "    # Plot 1: Labels ao longo do tempo\n",
    "    plt.subplot(1, 2, 1)\n",
    "    colors = [color_map[label] for label in labels]\n",
    "    plt.scatter(x, labels, c=colors, alpha=0.6, s=50)\n",
    "    plt.xlabel('Índice do Conjunto de Pontos', fontsize=12)\n",
    "    plt.ylabel('Cluster (Label Predito)', fontsize=12)\n",
    "    plt.title(f'K-means Clustering - Pessoa {pessoa_id}\\n(Cada ponto = {window_size} leituras)', fontsize=14)\n",
    "    plt.yticks(unique, [f'Cluster {i}' for i in unique])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Distribuição dos clusters\n",
    "    plt.subplot(1, 2, 2)\n",
    "    cluster_labels = [f'Cluster {i}' for i in unique]\n",
    "    plt.bar(cluster_labels, counts, color=color_palette, alpha=0.7)\n",
    "    plt.xlabel('Cluster', fontsize=12)\n",
    "    plt.ylabel('Número de Conjuntos de Pontos', fontsize=12)\n",
    "    plt.title('Distribuição dos Clusters', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, count in enumerate(counts):\n",
    "        plt.text(i, count, str(count), ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'clustering_pessoa_{pessoa_id}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ Gráfico salvo como 'clustering_pessoa_{pessoa_id}.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f0cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_pessoa(pessoa_id, window_size, n_clusters):\n",
    "    \"\"\"\n",
    "    Análise completa de clusterização para uma pessoa (COM DOWNSAMPLING)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ANÁLISE - PESSOA {pessoa_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 1. Carregar dados\n",
    "    print(\"\\n[1/6] Carregando dados...\")\n",
    "    df_accel, df_gyro = carregar_dados_pessoa(pessoa_id)\n",
    "    print(f\"   ✓ Acelerômetro: {len(df_accel)} pontos\")\n",
    "    print(f\"   ✓ Giroscópio: {len(df_gyro)} pontos\")\n",
    "\n",
    "    # 2. Sincronizar dados\n",
    "    print(\"\\n[2/6] Sincronizando dados (tolerância={TOLERANCIA_SINCRONIZACAO_S}s)...\")\n",
    "    df_combined = sincronizar_dados(df_accel, df_gyro, TOLERANCIA_SINCRONIZACAO_S)\n",
    "    print(f\"   ✓ Dados sincronizados: {len(df_combined)} pontos (taxa original)\")\n",
    "\n",
    "    # 3. NOVO: Realizar Downsampling\n",
    "    print(f\"\\n[3/6] Realizando Downsampling (frequência={FREQ_DOWNSAMPLE})...\")\n",
    "    df_final = realizar_downsample(df_combined, freq=FREQ_DOWNSAMPLE)\n",
    "    print(f\"   ✓ Pontos após downsample: {len(df_final)} pontos\")\n",
    "\n",
    "    # 4. Criar features em janelas (AGORA USANDO O DADO DOWNSAMPLED)\n",
    "    print(f\"\\n[4/6] Criando features (janelas de {window_size} pontos)...\")\n",
    "    # NOTA: Usamos 'df_final' aqui, não mais 'df_combined'\n",
    "    features = criar_features_janela(df_final, window_size)\n",
    "    n_features_per_window = window_size * 6\n",
    "    print(f\"   ✓ {len(features)} conjuntos de pontos criados\")\n",
    "    print(f\"   ✓ Cada conjunto representa {window_size} * {FREQ_DOWNSAMPLE}\")\n",
    "\n",
    "    # 5. Aplicar K-means\n",
    "    print(f\"\\n[5/6] Aplicando K-means (k={n_clusters})...\")\n",
    "    kmeans, labels, features_normalized = aplicar_kmeans(features, n_clusters)\n",
    "    print(f\"   ✓ Clustering concluído\")\n",
    "    print(f\"   ✓ Inércia: {kmeans.inertia_:.2f}\")\n",
    "\n",
    "    # 6. Plotar resultados\n",
    "    print(\"\\n[6/6] Gerando visualização...\")\n",
    "    plotar_resultados(labels, pessoa_id, window_size)\n",
    "\n",
    "    # Estatísticas\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ESTATÍSTICAS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    for cluster_id, count in zip(unique, counts):\n",
    "        percentage = (count / len(labels)) * 100\n",
    "        print(f\"Cluster {cluster_id}: {count} conjuntos ({percentage:.1f}%)\")\n",
    "\n",
    "    return df_final, features, labels, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef41cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validação rápida: verifica que a criação de features respeita WINDOW_SIZE\n",
    "# Executar após definir/rodar analisar_pessoa para validar comportamento\n",
    "try:\n",
    "    # Se as variáveis ainda não existirem no escopo, pedimos para executar as células anteriores primeiro\n",
    "    assert 'dados_sincronizados' in globals() or 'df_combined' in globals(), \\\n",
    "        \"Execute as células de carregamento/sincronização antes desta validação.\"\n",
    "\n",
    "    # Obter df usado: prefer df_combined criado pela última análise ou dados_sincronizados\n",
    "    df_to_use = globals().get('df_combined', globals().get('dados_sincronizados'))\n",
    "\n",
    "    # Gerar features de teste com WINDOW_SIZE\n",
    "    test_features = criar_features_janela(df_to_use, WINDOW_SIZE)\n",
    "\n",
    "    assert len(test_features) > 0, \"Nenhuma janela criada — verifique WINDOW_SIZE e o tamanho do dataframe.\"\n",
    "    assert len(test_features[0]) == WINDOW_SIZE * 6, \\\n",
    "        f\"Cada feature deve ter WINDOW_SIZE*6 elementos, esperado {WINDOW_SIZE*6}, obteve {len(test_features[0])}.\"\n",
    "\n",
    "    print(f\"✓ Validação: {len(test_features)} janelas criadas; cada janela tem {len(test_features[0])} features ({WINDOW_SIZE} pontos × 6).\")\n",
    "except AssertionError as ae:\n",
    "    print(f\"✗ Validação falhou: {ae}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Erro na validação: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a54143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_todas_pessoas(window_size, n_clusters):\n",
    "    \"\"\"\n",
    "    Análise de clusterização para todas as pessoas\n",
    "    \n",
    "    Args:\n",
    "        window_size: Número de pontos por janela\n",
    "        n_clusters: Número de clusters\n",
    "    \"\"\"\n",
    "    # IDs das pessoas disponíveis\n",
    "    pessoas = [11, 12, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, \n",
    "               29, 30, 31, 32, 33, 34, 36, 37, 38]\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANÁLISE DE TODAS AS PESSOAS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for pessoa_id in pessoas:\n",
    "        try:\n",
    "            df_combined, features, labels, kmeans = analisar_pessoa(\n",
    "                pessoa_id, window_size, n_clusters\n",
    "            )\n",
    "            resultados[pessoa_id] = {\n",
    "                'data': df_combined,\n",
    "                'features': features,\n",
    "                'labels': labels,\n",
    "                'kmeans': kmeans\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠ Erro ao processar pessoa {pessoa_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sumário geral\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"SUMÁRIO GERAL\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total de pessoas analisadas: {len(resultados)}/{len(pessoas)}\")\n",
    "    \n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ba515",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLUSTERIZAÇÃO K-MEANS - ACELERÔMETRO E GIROSCÓPIO\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Taxa de amostragem: {TOLERANCIA_SINCRONIZACAO_S}s\")\n",
    "    print(f\"Número de clusters: {N_CLUSTERS}\")\n",
    "    print(f\"Tamanho da janela: {WINDOW_SIZE} pontos (leituras por janela)\")\n",
    "    \n",
    "    # Opção 1: Analisar apenas uma pessoa (recomendado para começar)\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Opção escolhida: Analisar uma pessoa primeiro\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    df_combined, features, labels, kmeans = analisar_pessoa(\n",
    "        PESSOA_INICIAL, \n",
    "        window_size=WINDOW_SIZE, \n",
    "        n_clusters=N_CLUSTERS\n",
    "    )\n",
    "    \n",
    "    # Descomente a linha abaixo para analisar todas as pessoas\n",
    "    # resultados = analisar_todas_pessoas(window_size=N_JANELAS, n_clusters=N_CLUSTERS)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANÁLISE CONCLUÍDA!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nPara analisar todas as pessoas, descomente a última linha do script.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
